
<!DOCTYPE HTML>
<html lang="" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>Going Streaming: When and How Â· GitBook</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="summary.html" />
    
    
    <link rel="prev" href="section1.2.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="Type to search" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="../INTRODUCTION.html">
            
                <a href="../INTRODUCTION.html">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../chapter1/">
            
                <a href="../chapter1/">
            
                    
                    Chapter1 Streaming 101
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" data-path="../chapter1/section1.1.html">
            
                <a href="../chapter1/section1.1.html">
            
                    
                    Section1.1 Terminology: What Is Streaming?
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2" data-path="../chapter1/section1.2.html">
            
                <a href="../chapter1/section1.2.html">
            
                    
                    Section1.2 Data Processing Patterns
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3" data-path="../chapter1/summary.html">
            
                <a href="../chapter1/summary.html">
            
                    
                    Summary
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="./">
            
                <a href="./">
            
                    
                    Chapter2 The What, Where,When, and How of Data Processing
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.4.1" data-path="../chapter1/section1.1.html">
            
                <a href="../chapter1/section1.1.html">
            
                    
                    Section2.1 Roadmap
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.2" data-path="section1.2.html">
            
                <a href="section1.2.html">
            
                    
                    Section2.2 Batch Foundations: What and Where
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.4.3" data-path="summary0.html">
            
                <a href="summary0.html">
            
                    
                    Going Streaming: When and How
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4.4" data-path="summary.html">
            
                <a href="summary.html">
            
                    
                    Summary
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="../chapter3/">
            
                <a href="../chapter3/">
            
                    
                    Chapter3  Watermarks
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.5.1" data-path="../chapter3/section1.html">
            
                <a href="../chapter3/section1.html">
            
                    
                    Section3.1 Definition
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.2" data-path="../chapter3/section2.html">
            
                <a href="../chapter3/section2.html">
            
                    
                    Section3.2 Source Watermark Creation
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.3" data-path="../chapter3/section3.html">
            
                <a href="../chapter3/section3.html">
            
                    
                    Watermark Propagation
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.4" data-path="../chapter3/section4.html">
            
                <a href="../chapter3/section4.html">
            
                    
                    Percentile Watermarks
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.5" data-path="../chapter3/section5.html">
            
                <a href="../chapter3/section5.html">
            
                    
                    Processing-Time Watermarks
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.6" data-path="../chapter3/section6.html">
            
                <a href="../chapter3/section6.html">
            
                    
                    Case Studies
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.7" data-path="../chapter3/section7.html">
            
                <a href="../chapter3/section7.html">
            
                    
                    Watermark Propagation
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.8" data-path="../chapter3/summary.md">
            
                <span>
            
                    
                    Summary
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.6" data-path="../chapter4/">
            
                <a href="../chapter4/">
            
                    
                    Chapter 4. Advanced Windowing
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.6.1" data-path="../chapter4/section1.html">
            
                <a href="../chapter4/section1.html">
            
                    
                    When/Where: Processing-Time Windows
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.2" data-path="../chapter4/section2.html">
            
                <a href="../chapter4/section2.html">
            
                    
                    Where: Session Windows
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.3" data-path="../chapter4/section3.html">
            
                <a href="../chapter4/section3.html">
            
                    
                    Where: Custom Windowing
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.6.4" data-path="../chapter4/summary.html">
            
                <a href="../chapter4/summary.html">
            
                    
                    Summary
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.7" data-path="../chapter5/">
            
                <a href="../chapter5/">
            
                    
                    Chapter5 Exactly-Once and Side Effects
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.7.1" data-path="../chapter5/section1.html">
            
                <a href="../chapter5/section1.html">
            
                    
                    Why Exactly Once Matters
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.2" data-path="../chapter5/section2.html">
            
                <a href="../chapter5/section2.html">
            
                    
                    Accuracy Versus Completeness
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.3" data-path="../chapter5/section3.html">
            
                <a href="../chapter5/section3.html">
            
                    
                    Ensuring Exactly Once in Shuffle
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.4" data-path="../chapter5/section4.html">
            
                <a href="../chapter5/section4.html">
            
                    
                    Addressing Determinism
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.5" data-path="../chapter5/section5.html">
            
                <a href="../chapter5/section5.html">
            
                    
                    Performance
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.6" data-path="../chapter5/section6.html">
            
                <a href="../chapter5/section6.html">
            
                    
                    Exactly Once in Sources
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.7" data-path="../chapter5/section7.html">
            
                <a href="../chapter5/section7.html">
            
                    
                    Exactly Once in Sinks
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.8" data-path="../chapter5/section8.html">
            
                <a href="../chapter5/section8.html">
            
                    
                    Use Cases
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.9" data-path="../chapter5/section8.html">
            
                <a href="../chapter5/section8.html">
            
                    
                    Other Systems
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.7.10" data-path="../chapter5/section9.html">
            
                <a href="../chapter5/section9.html">
            
                    
                    Summary
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.8" data-path="../chapter6/">
            
                <a href="../chapter6/">
            
                    
                    Chapter6 Streams and Tables
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.8.1" data-path="../chapter6/section1.html">
            
                <a href="../chapter6/section1.html">
            
                    
                    Stream-and-Table Basics Or: a Special Theory of Stream and Table Relativity
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8.2" data-path="../chapter6/section2.html">
            
                <a href="../chapter6/section2.html">
            
                    
                    Batch Processing Versus Streams and Tables
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8.3" data-path="../chapter6/section3.html">
            
                <a href="../chapter6/section3.html">
            
                    
                    What, Where, When, and How in a Streams and Tables World
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8.4" data-path="../chapter6/section4.html">
            
                <a href="../chapter6/section4.html">
            
                    
                    A General Theory of Stream and Table Relativity
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.8.5" data-path="../chapter6/section5.html">
            
                <a href="../chapter6/section5.html">
            
                    
                    Summary
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.9" data-path="../chapter7/">
            
                <a href="../chapter7/">
            
                    
                    Chapter 7. The Practicalities of Persistent State
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.9.1" data-path="../chapter7/section1.html">
            
                <a href="../chapter7/section1.html">
            
                    
                    Motivation
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.2" data-path="../chapter7/section2.html">
            
                <a href="../chapter7/section2.html">
            
                    
                    Implicit State
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.3" data-path="../chapter7/section3.html">
            
                <a href="../chapter7/section3.html">
            
                    
                    Generalized State
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.9.4" data-path="../chapter7/section4.html">
            
                <a href="../chapter7/section4.html">
            
                    
                    Summary
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.10" data-path="../chapter8/">
            
                <a href="../chapter8/">
            
                    
                    Chapter 8. Streaming SQL
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.10.1" data-path="../chapter8/section1.html">
            
                <a href="../chapter8/section1.html">
            
                    
                    What Is Streaming SQL?
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.10.2" data-path="../chapter8/section2.html">
            
                <a href="../chapter8/section2.html">
            
                    
                    Looking Backward: Stream and Table Biases
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.10.3" data-path="../chapter8/section3.html">
            
                <a href="../chapter8/section3.html">
            
                    
                    Looking Forward: Toward Robust Streaming SQL
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.10.4" data-path="../chapter8/section4.html">
            
                <a href="../chapter8/section4.html">
            
                    
                    Summary
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.11" data-path="../chapter9/">
            
                <a href="../chapter9/">
            
                    
                    Chapter 9. Streaming Joins
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.11.1" data-path="../chapter9/section2.html">
            
                <a href="../chapter9/section2.html">
            
                    
                    All Your Joins Are Belong to Streaming
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.11.2" data-path="../chapter9/section3.html">
            
                <a href="../chapter9/section3.html">
            
                    
                    Unwindowed Joins
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.11.3" data-path="../chapter9/section4.html">
            
                <a href="../chapter9/section4.html">
            
                    
                    Windowed Joins
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.11.4" data-path="../chapter9/section5.html">
            
                <a href="../chapter9/section5.html">
            
                    
                    Summary
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.12" data-path="../chapter10/">
            
                <a href="../chapter10/">
            
                    
                    Chapter 10. The Evolution of Large-Scale Data Processing
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.12.1" data-path="../chapter10/section1.html">
            
                <a href="../chapter10/section1.html">
            
                    
                    MapReduce
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.12.2" data-path="../chapter10/section2.html">
            
                <a href="../chapter10/section2.html">
            
                    
                    Hadoop
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.12.3" data-path="../chapter10/section3.html">
            
                <a href="../chapter10/section3.html">
            
                    
                    Flume
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.12.4" data-path="../chapter10/section4.html">
            
                <a href="../chapter10/section4.html">
            
                    
                    Storm
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.12.5" data-path="../chapter10/section5.html">
            
                <a href="../chapter10/section5.html">
            
                    
                    Spark
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.12.6" data-path="../chapter10/section6.html">
            
                <a href="../chapter10/section6.html">
            
                    
                    MillWheel
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.12.7" data-path="../chapter10/section7.html">
            
                <a href="../chapter10/section7.html">
            
                    
                    Kafaka
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.12.8" data-path="../chapter10/section8.html">
            
                <a href="../chapter10/section8.html">
            
                    
                    CloudDataFlow
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.12.9" data-path="../chapter10/section9.html">
            
                <a href="../chapter10/section9.html">
            
                    
                    Flink
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.12.10" data-path="../chapter10/section9.html">
            
                <a href="../chapter10/section9.html">
            
                    
                    Beam
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.12.11" data-path="../chapter10/section10.html">
            
                <a href="../chapter10/section10.html">
            
                    
                    Summary
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            Published with GitBook
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >Going Streaming: When and How</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h2 id="going-streaming-when-and-how">Going Streaming: When and How</h2>
<p>We just observed the execution of a windowed pipeline on a batch engine.
But, ideally, we&#x2019;d like to have lower latency for our results, and we&#x2019;d also
like to natively handle unbounded data sources. Switching to a streaming
engine is a step in the right direction, but our previous strategy of waiting
until our input has been consumed in its entirety to generate output is no
longer feasible. Enter triggers and watermarks.</p>
<h2 id="when--the-wonderful-thing-about-triggers-is">When : The Wonderful Thing About Triggers Is</h2>
<h2 id="triggers-are-wonderful-things">Triggers Are Wonderful Things!</h2>
<p>Triggers provide the answer to the question: &#x201C; <em>When</em> in processing time are
results materialized?&#x201D; Triggers declare when output for a window should
happen in processing time (though the triggers themselves might make those
decisions based on things that happen in other time domains, such as</p>
<pre><code>00:00 / 00:00
</code></pre><p>watermarks progressing in the event-time domain, as we&#x2019;ll see in a few
moments). Each specific output for a window is referred to as a <em>pane</em> of the
window.</p>
<p>Though it&#x2019;s possible to imagine quite a breadth of possible triggering
semantics, conceptually there are only two generally useful types of triggers,
and practical applications almost always boil down using either one or a
combination of both:</p>
<p>Repeated update triggers</p>
<pre><code>These periodically generate updated panes for a window as its contents
evolve. These updates can be materialized with every new record, or they
can happen after some processing-time delay, such as once a minute. The
choice of period for a repeated update trigger is primarily an exercise in
balancing latency and cost.
</code></pre><p>Completeness triggers</p>
<pre><code>These materialize a pane for a window only after the input for that
window is believed to be complete to some threshold. This type of trigger
is most analogous to what we&#x2019;re familiar with in batch processing: only
after the input is complete do we provide a result. The difference in the
trigger-based approach is that the notion of completeness is scoped to the
context of a single window, rather than always being bound to the
completeness of the entire input.
</code></pre><p>Repeated update triggers are the most common type of trigger encountered in
streaming systems. They are simple to implement and simple to understand,
and they provide useful semantics for a specific type of use case: repeated
(and eventually consistent) updates to a materialized dataset, analogous to the
semantics you get with materialized views in the database world.</p>
<p>Completeness triggers are less frequently encountered, but provide streaming
semantics that more closely align with those from the classic batch
processing world. They also provide tools for reasoning about things like
missing data and late data, both of which we discuss shortly (and in the next
chapter) as we explore the underlying primitive that drives completeness</p>
<pre><code>3
</code></pre><p>triggers: watermarks.</p>
<p>But first, let&#x2019;s start simple and look at some basic repeated update triggers in
action. To make the notion of triggers a bit more concrete, let&#x2019;s go ahead and
add the most straightforward type of trigger to our example pipeline: a trigger
that fires with every new record, as shown in Example 2-3.</p>
<p><em>Example 2-3. Triggering repeatedly with every record</em></p>
<p>PCollection<kv<team, integer="">&gt; totals = input
.apply(Window.into(FixedWindows.of(TWO_MINUTES))
.triggering(Repeatedly(AfterCount(1))));
.apply(Sum.integersPerKey());</kv<team,></p>
<p>If we were to run this new pipeline on a streaming engine, the results would
look something like that shown in Figure 2-6.</p>
<pre><code>Figure 2-6. Per-record triggering on a streaming engine
</code></pre><p>You can see how we now get multiple outputs (panes) for each window: once
per corresponding input. This sort of triggering pattern works well when the
output stream is being written to some sort of table that you can simply poll
for results. Any time you look in the table, you&#x2019;ll see the most up-to-date
value for a given window, and those values will converge toward correctness
over time.</p>
<p>One downside of per-record triggering is that it&#x2019;s quite chatty. When
processing large-scale data, aggregations like summation provide a nice
opportunity to reduce the cardinality of the stream without losing
information. This is particularly noticeable for cases in which you have high-
volume keys; for our example, massive teams with lots of active players.
Imagine a massively multiplayer game in which players are split into one of</p>
<pre><code>00:00 / 00:00
</code></pre><p>two factions, and you want to tally stats on a per-faction basis. It&#x2019;s probably
unnecessary to update your tallies with every new input record for every
player in a given faction. Instead, you might be happy updating them after
some processing-time delay, say every second, or every minute. The nice side
effect of using processing-time delays is that it has an equalizing effect across
high-volume keys or windows: the resulting stream ends up being more
uniform cardinality-wise.</p>
<p>There are two different approaches to processing-time delays in triggers:
<em>aligned delays</em> (where the delay slices up processing time into fixed regions
that align across keys and windows) and <em>unaligned delays</em> (where the delay is
relative to the data observed within a given window). A pipeline with
unaligned delays might look like Example 2-4, the results of which are shown
in Figure 2-7.</p>
<p><em>Example 2-4. Triggering on aligned two-minute processing-time boundaries</em></p>
<p>PCollection<kv<team, integer="">&gt; totals = input
.apply(Window.into(FixedWindows.of(TWO_MINUTES))
.triggering(Repeatedly(AlignedDelay(TWO_MINUTES)))
.apply(Sum.integersPerKey());</kv<team,></p>
<pre><code>Figure 2-7. Two-minute aligned delay triggers (i.e., microbatching)
</code></pre><p>This sort of aligned delay trigger is effectively what you get from a
microbatch streaming system like Spark Streaming. The nice thing about it is
predictability; you get regular updates across all modified windows at the
same time. That&#x2019;s also the downside: all updates happen at once, which
results in bursty workloads that often require greater peak provisioning to
properly handle the load. The alternative is to use an unaligned delay. That
would look something Example 2-5 in Beam. Figure 2-8 presents the results.</p>
<pre><code>00:00 / 00:00
</code></pre><p><em>Example 2-5. Triggering on unaligned two-minute processing-time
boundaries</em></p>
<p>PCollection<kv<team, integer="">&gt; totals = input
.apply(Window.into(FixedWindows.of(TWO_MINUTES))
.triggering(Repeatedly(UnalignedDelay(TWO_MINUTES))
.apply(Sum.integersPerKey());</kv<team,></p>
<pre><code>Figure 2-8. Two-minute unaligned delay triggers
</code></pre><p>Contrasting the unaligned delays in Figure 2-8 to the aligned delays in
Figure 2-6, it&#x2019;s easy to see how the unaligned delays spread the load out more
evenly across time. The actual latencies involved for any given window differ
between the two, sometimes more and sometimes less, but in the end the
average latency will remain essentially the same. From that perspective,
unaligned delays are typically the better choice for large-scale processing
because they result in a more even load distribution over time.</p>
<p>Repeated update triggers are great for use cases in which we simply want
periodic updates to our results over time and are fine with those updates
converging toward correctness with no clear indication of when correctness is
achieved. However, as we discussed in Chapter 1, the vagaries of distributed
systems often lead to a varying level of skew between the time an event
happens and the time it&#x2019;s actually observed by your pipeline, which means it
can be difficult to reason about when your output presents an accurate and
complete view of your input data. For cases in which input completeness
matters, it&#x2019;s important to have some way of reasoning about completeness
rather than blindly trusting the results calculated by whichever subset of data
happen to have found their way to your pipeline. Enter watermarks.</p>
<pre><code>00:00 / 00:00
</code></pre><h2 id="when--watermarks">When : Watermarks</h2>
<p>Watermarks are a supporting aspect of the answer to the question: &#x201C; <em>When</em> in
processing time are results materialized?&#x201D; Watermarks are temporal notions
of input completeness in the event-time domain. Worded differently, they are
the way the system measures progress and completeness relative to the event
times of the records being processed in a stream of events (either bounded or
unbounded, though their usefulness is more apparent in the unbounded case).</p>
<p>Recall this diagram from Chapter 1, slightly modified in Figure 2-9, in which
I described the skew between event time and processing time as an ever-
changing function of time for most real-world distributed data processing
systems.</p>
<pre><code>Figure 2-9. Event-time progress, skew, and watermarks
</code></pre><p>That meandering red line that I claimed represented reality is essentially the
watermark; it captures the progress of event-time completeness as processing
time progresses. Conceptually, you can think of the watermark as a function,
<em>F</em> ( <em>P</em> ) &#x2192; <em>E</em> , which takes a point in processing time and returns a point in event
time. That point in event time, <em>E</em> , is the point up to which the system
believes all inputs with event times less than <em>E</em> have been observed. In other
words, it&#x2019;s an assertion that no more data with event times less than <em>E</em> will</p>
<pre><code>4
</code></pre><p>ever be seen again. Depending upon the type of watermark, perfect or
heuristic, that assertion can be a strict guarantee or an educated guess,
respectively:</p>
<p>Perfect watermarks</p>
<pre><code>For the case in which we have perfect knowledge of all of the input data,
it&#x2019;s possible to construct a perfect watermark. In such a case, there is no
such thing as late data; all data are early or on time.
</code></pre><p>Heuristic watermarks</p>
<pre><code>For many distributed input sources, perfect knowledge of the input data is
impractical, in which case the next best option is to provide a heuristic
watermark. Heuristic watermarks use whatever information is available
about the inputs (partitions, ordering within partitions if any, growth rates
of files, etc.) to provide an estimate of progress that is as accurate as
possible. In many cases, such watermarks can be remarkably accurate in
their predictions. Even so, the use of a heuristic watermark means that it
might sometimes be wrong, which will lead to late data. We show you
about ways to deal with late data soon.
</code></pre><p>Because they provide a notion of completeness relative to our inputs,
watermarks form the foundation for the second type of trigger mentioned
previously: <em>completeness triggers</em>. Watermarks themselves are a fascinating
and complex topic, as you&#x2019;ll see when you get to Slava&#x2019;s watermarks deep
dive in Chapter 3. But for now, let&#x2019;s look at them in action by updating our
example pipeline to utilize a completeness trigger built upon watermarks, as
demonstrated in Example 2-6.</p>
<p><em>Example 2-6. Watermark completeness trigger</em></p>
<p>PCollection<kv<team, integer="">&gt; totals = input
.apply(Window.into(FixedWindows.of(TWO_MINUTES))
.triggering(AfterWatermark()))
.apply(Sum.integersPerKey());</kv<team,></p>
<p>Now, an interesting quality of watermarks is that they are a class of functions,
meaning there are multiple different functions <em>F</em> ( <em>P</em> ) &#x2192; <em>E</em> that satisfy the
properties of a watermark, to varying degrees of success. As I noted earlier,</p>
<p>for situations in which you have perfect knowledge of your input data, it
might be possible to build a perfect watermark, which is the ideal situation.
But for cases in which you lack perfect knowledge of the inputs or for which
it&#x2019;s simply too computationally expensive to calculate the perfect watermark,
you might instead choose to utilize a heuristic for defining your watermark.
The point I want to make here is that the given watermark algorithm in use is
independent from the pipeline itself. We&#x2019;re not going to discuss in detail what
it means to implement a watermark here (Slava does that in Chapter 3). For
now, to help drive home this idea that a given input set can have different
watermarks applied to it, let&#x2019;s take a look at our pipeline in Example 2-6
when executed on the same dataset but using two distinct watermark
implementations (Figure 2-10): on the left, a perfect watermark; on the right,
a heuristic watermark.</p>
<p>In both cases, windows are materialized as the watermark passes the end of
the window. The perfect watermark, as you might expect, perfectly captures
the event-time completeness of the pipeline as time progresses. In contrast,
the specific algorithm used for the heuristic watermark on the right fails to
take the value of 9 into account, which drastically changes the shape of the
materialized outputs, both in terms of output latency and correctness (as seen
by the incorrect answer of 5 that&#x2019;s provided for the [12:00, 12:02) window).</p>
<p>The big difference between the watermark triggers from Figure 2-9 and the
repeated update triggers we saw in Figures 2-5 through 2-7 is that the
<em>watermarks give us a way to reason about the completeness of our input</em>.
Until the system materializes an output for a given window, we know that the
system does not yet believe the inputs to be complete. This is especially
important for use cases in which you want to reason about a <em>lack of data</em> in
the input, or <em>missing data</em>.</p>
<pre><code>5
</code></pre><pre><code>00:00 / 00:00
</code></pre><pre><code>Figure 2-10. Windowed summation on a streaming engine with perfect (left) and heuristic
(right) watermarks
</code></pre><p>A great example of a missing-data use case is outer joins. Without a notion of
completeness like watermarks, how do you know when to give up and emit a
partial join rather than continue to wait for that join to complete? You don&#x2019;t.
And basing that decision on a processing-time delay, which is the common
approach in streaming systems that lack true watermark support, is not a safe
way to go, because of the variable nature of event-time skew we spoke about
in Chapter 1: as long as skew remains smaller than the chosen processing-
time delay, your missing-data results will be correct, but any time skew
grows beyond that delay, they will suddenly become <em>in</em> correct. From this
perspective, event-time watermarks are a critical piece of the puzzle for many
real-world streaming use cases which must reason about a lack of data in the
input, such as outer joins, anomaly detection, and so on.</p>
<p>Now, with that said, these watermark examples also highlight two
<em>shortcomings</em> of watermarks (and any other notion of completeness),
specifically that they can be one of the following:</p>
<p>Too slow</p>
<pre><code>When a watermark of any type is correctly delayed due to known
unprocessed data (e.g., slowly growing input logs due to network
bandwidth constraints), that translates directly into delays in output if
advancement of the watermark is the only thing you depend on for
stimulating results.
This is most obvious in the left diagram of Figure 2-10, for which the late
arriving 9 holds back the watermark for all the subsequent windows, even
though the input data for those windows become complete earlier. This is
particularly apparent for the second window, [12:02, 12:04), for which it
takes nearly seven minutes from the time the first value in the window
occurs until we see any results for the window whatsoever. The heuristic
watermark in this example doesn&#x2019;t suffer the same issue quite so badly
(five minutes until output), but don&#x2019;t take that to mean heuristic
watermarks never suffer from watermark lag; that&#x2019;s really just a
</code></pre><pre><code>consequence of the record I chose to omit from the heuristic watermark in
this specific example.
The important point here is the following: Although watermarks provide
a very useful notion of completeness, depending upon completeness for
producing output is often not ideal from a latency perspective. Imagine a
dashboard that contains valuable metrics, windowed by hour or day. It&#x2019;s
unlikely you&#x2019;d want to wait a full hour or day to begin seeing results for
the current window; that&#x2019;s one of the pain points of using classic batch
systems to power such systems. Instead, it would be much nicer to see the
results for those windows refine over time as the inputs evolve and
eventually become complete.
</code></pre><p>Too fast</p>
<pre><code>When a heuristic watermark is incorrectly advanced earlier than it should
be, it&#x2019;s possible for data with event times before the watermark to arrive
some time later, creating late data. This is what happened in the example
on the right: the watermark advanced past the end of the first window
before all the input data for that window had been observed, resulting in
an incorrect output value of 5 instead of 14. This shortcoming is strictly a
problem with heuristic watermarks; their heuristic nature implies they
will sometimes be wrong. As a result, relying on them alone for
determining when to materialize output is insufficient if you care about
correctness.
</code></pre><p>In Chapter 1, I made some rather emphatic statements about notions of
completeness being insufficient for most use cases requiring robust out-of-
order processing of unbounded data streams. These two shortcomings&#x2014;
watermarks being too slow or too fast&#x2014;are the foundations for those
arguments. You simply cannot get both low latency and correctness out of a
system that relies solely on notions of completeness. So, for cases for which
you do want the best of both worlds, what&#x2019;s a person to do? Well, if repeated
update triggers provide low-latency updates but no way to reason about
completeness, and watermarks provide a notion of completeness but variable
and possible high latency, why not combine their powers together?</p>
<pre><code>6
</code></pre><h2 id="when--earlyon-timelate-triggers-ftw">When : Early/On-Time/Late Triggers FTW!</h2>
<p>We&#x2019;ve now looked at the two main types of triggers: repeated update triggers
and completeness/watermark triggers. In many case, neither of them alone is
sufficient, but the combination of them together is. Beam recognizes this fact
by providing an extension of the standard watermark trigger that also
supports repeated update triggering on either side of the watermark. This is
known as the early/on-time/late trigger because it partitions the panes that are
materialized by the compound trigger into three categories:</p>
<pre><code>Zero or more early panes , which are the result of a repeated update
trigger that periodically fires up until the watermark passes the end
of the window. The panes generated by these firings contain
speculative results, but allow us to observe the evolution of the
window over time as new input data arrive. This compensates for the
shortcoming of watermarks sometimes being too slow.
</code></pre><pre><code>A single on-time pane , which is the result of the
completeness/watermark trigger firing after the watermark passes the
end of the window. This firing is special because it provides an
assertion that the system now believes the input for this window to
be complete. This means that it is now safe to reason about missing
data ; for example, to emit a partial join when performing an outer
join.
</code></pre><pre><code>Zero or more late panes , which are the result of another (possibly
different) repeated update trigger that periodically fires any time late
data arrive after the watermark has passed the end of the window. In
the case of a perfect watermark, there will always be zero late panes.
But in the case of a heuristic watermark, any data the watermark
failed to properly account for will result in a late firing. This
compensates for the shortcoming of watermarks being too fast.
</code></pre><p>Let&#x2019;s see how this looks in action. We&#x2019;ll update our pipeline to use a periodic
processing-time trigger with an aligned delay of one minute for the early
firings, and a per-record trigger for the late firings. That way, the early firings</p>
<pre><code>7
</code></pre><p>will give us some amount of batching for high-volume windows (thanks to
the fact that the trigger will fire only once per minute, regardless of the
throughput into the window), but we won&#x2019;t introduce unnecessary latency for
the late firings, which are hopefully somewhat rare if we&#x2019;re using a
reasonably accurate heuristic watermark. In Beam, that looks Example 2-7
(Figure 2-11 shows the results).</p>
<p><em>Example 2-7. Early, on-time, and late firings via the early/on-time/late API</em></p>
<p>PCollection<kv<team, integer="">&gt; totals = input
.apply(Window.into(FixedWindows.of(TWO_MINUTES))
.triggering(AfterWatermark()
.withEarlyFirings(AlignedDelay(ONE_MINUTE))
.withLateFirings(AfterCount(1))))
.apply(Sum.integersPerKey());</kv<team,></p>
<pre><code>Figure 2-11. Windowed summation on a streaming engine with early, on-time, and late
firings
</code></pre><p>This version has two clear improvements over Figure 2-9:</p>
<pre><code>For the &#x201C;watermarks too slow&#x201D; case in the second window, [12:02,
12:04): we now provide periodic early updates once per minute. The
difference is most stark in the perfect watermark case, for which
time-to-first-output is reduced from almost seven minutes down to
three and a half; but it&#x2019;s also clearly improved in the heuristic case,
as well. Both versions now provide steady refinements over time
(panes with values 7, 10, then 18), with relatively minimal latency
between the input becoming complete and materialization of the
final output pane for the window.
</code></pre><pre><code>For the &#x201C;heuristic watermarks too fast&#x201D; case in the first window,
</code></pre><pre><code>00:00 / 00:00
</code></pre><pre><code>[12:00, 12:02): when the value of 9 shows up late, we immediately
incorporate it into a new, corrected pane with value of 14.
</code></pre><p>One interesting side effect of these new triggers is that they effectively
normalize the output pattern between the perfect and heuristic watermark
versions. Whereas the two versions in Figure 2-10 were starkly different, the
two versions here look quite similar. They also look much more similar to the
various repeated update version from Figures 2-6 through 2-8, with one
important difference: thanks to the use of the watermark trigger, we can also
reason about input completeness in the results we generate with the early/on-
time/late trigger. This allows us to better handle use cases that care about
<em>missing data</em> , like outer joins, anomaly detection, and so on.</p>
<p>The biggest remaining difference between the perfect and heuristic early/on-
time/late versions at this point is window lifetime bounds. In the perfect
watermark case, we know we&#x2019;ll never see any more data for a window after
the watermark has passed the end of it, hence we can drop all of our state for
the window at that time. In the heuristic watermark case, we still need to hold
on to the state for a window for some amount of time to account for late data.
But as of yet, our system doesn&#x2019;t have any good way of knowing just how
long state needs to be kept around for each window. That&#x2019;s where <em>allowed
lateness</em> comes in.</p>
<h2 id="when--allowed-lateness-ie-garbage-collection">When : Allowed Lateness (i.e., Garbage Collection)</h2>
<p>Before moving on to our last question (&#x201C; <em>How</em> do refinements of results
relate?&#x201D;), I&#x2019;d like to touch on a practical necessity within long-lived, out-of-
order stream processing systems: garbage collection. In the heuristic
watermarks example in Figure 2-11, the persistent state for each window
lingers around for the entire lifetime of the example; this is necessary to
allow us to appropriately deal with late data when/if they arrive. But while it
would be great to be able to keep around all of our persistent state until the
end of time, in reality, when dealing with an unbounded data source, it&#x2019;s
often not practical to keep state (including metadata) for a given window
indefinitely; we&#x2019;ll eventually run out of disk space (or at the very least tire of</p>
<p>paying for it, as the value for older data diminishes over time).</p>
<p>As a result, any real-world out-of-order processing system needs to provide
some way to bound the lifetimes of the windows it&#x2019;s processing. A clean and
concise way of doing this is by defining a horizon on the allowed lateness
within the system; that is, placing a bound on how late any given <em>record</em> may
be (relative to the watermark) for the system to bother processing it; any data
that arrives after this horizon are simply dropped. After you&#x2019;ve bounded how
late individual data may be, you&#x2019;ve also established precisely how long the
state for windows must be kept around: until the watermark exceeds the
lateness horizon for the end of the window. But in addition, you&#x2019;ve also given
the system the liberty to immediately drop any data later than the horizon as
soon as they&#x2019;re observed, which means the system doesn&#x2019;t waste resources
processing data that no one cares about.</p>
<h3 id="measuring-lateness">MEASURING LATENESS</h3>
<pre><code>It might seem a little odd to be specifying a horizon for handling late data
using the very metric that resulted in the late data in the first place (i.e.,
the heuristic watermark). And in some sense it is. But of the options
available, it&#x2019;s arguably the best. The only other practical option would be
to specify the horizon in processing time (e.g., keep windows around for
10 minutes of processing time after the watermark passes the end of the
window), but using processing time would leave the garbage collection
policy vulnerable to issues within the pipeline itself (e.g., workers
crashing, causing the pipeline to stall for a few minutes), which could
lead to windows that didn&#x2019;t actually have a chance to handle late data that
they otherwise should have. By specifying the horizon in the event-time
domain, garbage collection is directly tied to the actual progress of the
pipeline, which decreases the likelihood that a window will miss its
opportunity to handle late data appropriately.
Note however, that not all watermarks are created equal. When we speak
of watermarks in this book, we generally refer to low watermarks, which
pessimistically attempt to capture the event time of the oldest
unprocessed record the system is aware of. The nice thing about dealing
</code></pre><pre><code>with lateness via low watermarks is that they are resilient to changes in
event-time skew; no matter how large the skew in a pipeline may grow,
the low watermark will always track the oldest outstanding event known
to the system, providing the best guarantee of correctness possible.
In contrast, some systems may use the term &#x201C;watermark&#x201D; to mean other
things. For example, watermarks in Spark Structured Streaming are high
watermarks, which optimistically track the event time of the newest
record the system is aware of. When dealing with lateness, the system is
free to garbage collect any window older than the high watermark
adjusted by some user-specified lateness threshold. In other words, the
system allows you to specify the maximum amount of event-time skew
you expect to see in your pipeline, and then throws away any data outside
of that skew window. This can work well if skew within your pipeline
remains within some constant delta, but is more prone to incorrectly
discarding data than low watermarking schemes.
</code></pre><p>Because the interaction between allowed lateness and the watermark is a little
subtle, it&#x2019;s worth looking at an example. Let&#x2019;s take the heuristic watermark
pipeline from Example 2-7/Figure 2-11 and add in Example 2-8 a lateness
horizon of one minute (note that this particular horizon has been chosen
strictly because it fits nicely into the diagram; for real-world use cases, a
larger horizon would likely be much more practical):</p>
<p><em>Example 2-8. Early/on-time/late firings with allowed lateness</em></p>
<p>PCollection<kv<team, integer="">&gt; totals = input
.apply(Window.into(FixedWindows.of(TWO_MINUTES))
.triggering(
AfterWatermark()
.withEarlyFirings(AlignedDelay(ONE_MINUTE))
.withLateFirings(AfterCount(1)))
.withAllowedLateness(ONE_MINUTE))
.apply(Sum.integersPerKey());</kv<team,></p>
<p>The execution of this pipeline would look something like Figure 2-12, in
which I&#x2019;ve added the following features to highlight the effects of allowed
lateness:</p>
<pre><code>The thick black line denoting the current position in processing time
is now annotated with ticks indicating the lateness horizon (in event
time) for all active windows.
When the watermark passes the lateness horizon for a window, that
window is closed, which means that all state for the window is
discarded. I leave around a dotted rectangle showing the extent of
time (in both domains) that the window covered when it was closed,
with a little tail extending to the right to denote the lateness horizon
for the window (for contrasting against the watermark).
For this diagram only, I&#x2019;ve added an additional late datum for the
first window with value 6. The 6 is late, but still within the allowed
lateness horizon and thus is incorporated into an updated result with
value 11. The 9, however, arrives beyond the lateness horizon, so it
is simply dropped.
</code></pre><pre><code>Figure 2-12. Allowed lateness with early/on-time/late firings
</code></pre><p>Two final side notes about lateness horizons:</p>
<pre><code>To be absolutely clear, if you happen to be consuming data from
sources for which perfect watermarks are available, there&#x2019;s no need
to deal with late data, and an allowed lateness horizon of zero
seconds will be optimal. This is what we saw in the perfect
watermark portion of Figure 2-10.
One noteworthy exception to the rule of needing to specify lateness
horizons, even when heuristic watermarks are in use, would be
something like computing global aggregates over all time for a
tractably finite number of keys (e.g., computing the total number of
</code></pre><pre><code>00:00 / 00:00
</code></pre><pre><code>visits to your site over all time, grouped by web browser family). In
this case, the number of active windows in the system is bounded by
the limited keyspace in use. As long as the number of keys remains
manageably low, there&#x2019;s no need to worry about limiting the lifetime
of windows via allowed lateness.
</code></pre><p>Practicality sated, let&#x2019;s move on to our fourth and final question.</p>
<h2 id="how--accumulation">How : Accumulation</h2>
<p>When triggers are used to produce multiple panes for a single window over
time, we find ourselves confronted with the last question: &#x201C; <em>How</em> do
refinements of results relate?&#x201D; In the examples we&#x2019;ve seen so far, each
successive pane is built upon the one immediately preceding it. However,
there are actually three different modes of accumulation:</p>
<p>Discarding</p>
<pre><code>Every time a pane is materialized, any stored state is discarded. This
means that each successive pane is independent from any that came
before. Discarding mode is useful when the downstream consumer is
performing some sort of accumulation itself; for example, when sending
integers into a system that expects to receive deltas that it will sum
together to produce a final count.
</code></pre><p>Accumulating</p>
<pre><code>As in Figures 2-6 through 2-11, every time a pane is materialized, any
stored state is retained, and future inputs are accumulated into the existing
state. This means that each successive pane builds upon the previous
panes. Accumulating mode is useful when later results can simply
overwrite previous results, such as when storing output in a key/value
store like HBase or Bigtable.
</code></pre><p>Accumulating and retracting</p>
<pre><code>This is like accumulating mode, but when producing a new pane, it also
produces independent retractions for the previous pane(s). Retractions
</code></pre><pre><code>8 9
</code></pre><pre><code>(combined with the new accumulated result) are essentially an explicit
way of saying &#x201C;I previously told you the result was X , but I was wrong.
Get rid of the X I told you last time, and replace it with Y .&#x201D; There are two
cases for which retractions are particularly helpful:
</code></pre><pre><code>When consumers downstream are regrouping data by a different
dimension , it&#x2019;s entirely possible the new value may end up keyed
differently from the previous value and thus end up in a different
group. In that case, the new value can&#x2019;t just overwrite the old value;
you instead need the retraction to remove the old value
</code></pre><pre><code>When dynamic windows (e.g., sessions, which we look at more
closely in a few moments) are in use, the new value might be
replacing more than one previous window, due to window merging.
In this case, it can be difficult to determine from the new window
alone which old windows are being replaced. Having explicit
retractions for the old windows makes the task straightforward. We
see an example of this in detail in Chapter 8.
</code></pre><p>The different semantics for each group are somewhat clearer when seen side-
by-side. Consider the two panes for the second window (the one with event-
time range [12:06, 12:08)) in Figure 2-11 (the one with early/on-time/late
triggers). Table 2-1 shows what the values for each pane would look like
across the three accumulation modes (with <em>accumulating</em> mode being the
specific mode used in Figure 2-11 itself).</p>
<p><em>Table 2-1. Comparing accumulation modes using the second window
from Figure 2-11</em></p>
<p>(^) <strong>Discarding Accumulating Accumulating &amp;
Retracting
Pane 1: inputs=[3]</strong> 3 3 3
<strong>Pane 2: inputs=[8, 1]</strong> 9 12 12, &#x2013;3
<strong>Value of final normal</strong></p>
<pre><code>pane 9 12 12
</code></pre><pre><code>Sum of all panes 12 15 12
</code></pre><p>Let&#x2019;s take a closer look at what&#x2019;s happening:</p>
<p>Discarding</p>
<pre><code>Each pane incorporates only the values that arrived during that specific
pane. As such, the final value observed does not fully capture the total
sum. However, if you were to sum all of the independent panes
themselves, you would arrive at a correct answer of 12. This is why
discarding mode is useful when the downstream consumer itself is
performing some sort of aggregation on the materialized panes.
</code></pre><p>Accumulating</p>
<pre><code>As in Figure 2-11, each pane incorporates the values that arrived during
that specific pane, plus all of the values from previous panes. As such, the
final value observed correctly captures the total sum of 12. If you were to
sum up the individual panes themselves, however, you&#x2019;d effectively be
double-counting the inputs from pane 1, giving you an incorrect total sum
of 15. This is why accumulating mode is most useful when you can
simply overwrite previous values with new values: the new value already
incorporates all of the data seen thus far.
</code></pre><p>Accumulating and retracting</p>
<pre><code>Each pane includes both a new accumulating mode value as well as a
retraction of the previous pane&#x2019;s value. As such, both the last value
observed (excluding retractions) as well as the total sum of all
materialized panes (including retractions) provide you with the correct
answer of 12. This is why retractions are so powerful.
</code></pre><p>Example 2-9 demonstrates discarding mode in action, illustrating the changes
we would make to Example 2-7:</p>
<p><em>Example 2-9. Discarding mode version of early/on-time/late firings</em></p>
<p>PCollection<kv<team, integer="">&gt; totals = input
.apply(Window.into(FixedWindows.of(TWO_MINUTES))
.triggering(
AfterWatermark()
.withEarlyFirings(AlignedDelay(ONE_MINUTE))
.withLateFirings(AtCount(1)))
.discardingFiredPanes())
.apply(Sum.integersPerKey());</kv<team,></p>
<p>Running again on a streaming engine with a heuristic watermark would
produce output like that shown in Figure 2-13.</p>
<pre><code>Figure 2-13. Discarding mode version of early/on-time/late firings on a streaming engine
</code></pre><p>Even though the overall shape of the output is similar to the accumulating
mode version from Figure 2-11, note how none of the panes in this discarding
version overlap. As a result, each output is independent from the others.</p>
<p>If we want to look at retractions in action, the change would be similar, as
shown in Example 2-10. ??? depicts the results.</p>
<p><em>Example 2-10. Accumulating and retracting mode version of early/on-
time/late firings</em></p>
<p>PCollection<kv<team, integer="">&gt; totals = input
.apply(Window.into(FixedWindows.of(TWO_MINUTES))
.triggering(
AfterWatermark()
.withEarlyFirings(AlignedDelay(ONE_MINUTE))
.withLateFirings(AtCount(1)))
.accumulatingAndRetractingFiredPanes())
.apply(Sum.integersPerKey());</kv<team,></p>
<pre><code>00:00 / 00:00
</code></pre><p>Accumulating and retracting mode version of early/late firings on a streaming
engine</p>
<p>Because the panes for each window all overlap, it&#x2019;s a little tricky to see the
retractions clearly. The retractions are indicated in red, which combines with
the overlapping blue panes to yield a slightly purplish color. I&#x2019;ve also
horizontally shifted the values of the two outputs within a given pane slightly
(and separated them with a comma) to make them easier to differentiate.</p>
<p>Figure 2-14 combines the final frames of Figures 2-9, 2-11 (heuristic only),
and side-by-side, providing a nice visual contrast of the three modes.</p>
<pre><code>Figure 2-14. Side-by-side comparison of accumulation modes
</code></pre><p>As you can imagine, the modes in the order presented (discarding,
accumulating, accumulating and retracting) are each successively more
expensive in terms of storage and computation costs. To that end, choice of
accumulation mode provides yet another dimension for making trade-offs
along the axes of correctness, latency, and cost.</p>
<h2 id="summary">Summary</h2>
<p>With this chapter complete, you now understand the basics of robust stream
processing and are ready to go forth into the world and do amazing things. Of</p>
<pre><code>00:00 / 00:00
</code></pre><p>course, there are eight more chapters anxiously waiting for your attention, so
hopefully you won&#x2019;t go forth like right now, this very minute. But regardless,
let&#x2019;s recap what we&#x2019;ve just covered, lest you forget any of it in your haste to
amble forward. First, the major concepts we touched upon:</p>
<p>Event time versus processing time</p>
<pre><code>The all-important distinction between when events occurred and when
they are observed by your data processing system.
</code></pre><p>Windowing</p>
<pre><code>The commonly utilized approach to managing unbounded data by slicing
it along temporal boundaries (in either processing time or event time,
though we narrow the definition of windowing in the Beam Model to
mean only within event time).
</code></pre><p>Triggers</p>
<pre><code>The declarative mechanism for specifying precisely when materialization
of output makes sense for your particular use case.
</code></pre><p>Watermarks</p>
<pre><code>The powerful notion of progress in event time that provides a means of
reasoning about completeness (and thus missing data) in an out-of-order
processing system operating on unbounded data.
</code></pre><p>Accumulation</p>
<pre><code>The relationship between refinements of results for a single window for
cases in which it&#x2019;s materialized multiple times as it evolves.
</code></pre><p>Second, the four questions we used to frame our exploration:</p>
<pre><code>What results are calculated? = transformations.
</code></pre><pre><code>Where in event time are results calculated? = windowing.
When in processing time are results materialized? = triggers plus
watermarks.
</code></pre><pre><code>How do refinements of results relate? = accumulation.
</code></pre><p>Third, to drive home the flexibility afforded by this model of stream
processing (because in the end, that&#x2019;s really what this is all about: balancing
competing tensions like correctness, latency, and cost), a recap of the major
variations in output we were able to achieve over the same dataset with only a
minimal amount of code change:</p>
<pre><code>Integer summation
Example 2-1 / Figure 2-3
</code></pre><pre><code>Integer summation
Fixed windows batch
Example 2-2 / Figure 2-5
</code></pre><pre><code>Integer summation
Fixed windows streaming
Repeated per-record
trigger
Example 2-3 / Figure 2-6
</code></pre><pre><code>Integer summation
Fixed windows streaming
Repeated aligned-delay
trigger
Example 2-4 / Figure 2-7
</code></pre><pre><code>Integer summation
Fixed windows streaming
Repeated unaligned-delay
trigger
Example 2-5 / Figure 2-8
</code></pre><pre><code>Integer summation
Fixed windows streaming
Heuristic watermark
trigger
Example 2-6 / Figure 2-10
</code></pre><pre><code>Integer summation
Fixed windows streaming
Early/on-time/late trigger
</code></pre><pre><code>Integer summation
Fixed windows streaming
Early/on-time/late trigger
</code></pre><pre><code>Integer summation
Fixed windows streaming
Early/on-time/late trigger
</code></pre><pre><code>Discarding
Example 2-9 / Figure 2-13
</code></pre><pre><code>Accumulating
Example 2-7 / Figure 2-11
</code></pre><pre><code>Accumulating and
Retracting
Example 2-10 / ???
</code></pre><p>All that said, at this point, we&#x2019;ve really looked at only one type of
windowing: fixed windowing in event time. As we know, there are a number
of dimensions to windowing, and I&#x2019;d like to touch upon at least two more of
those before we call it day with the Beam Model. First, however, we&#x2019;re going
to take a slight detour to dive deeper into the world of watermarks, as this
knowledge will help frame future discussions (and be fascinating in and of
itself). Enter Slava, stage right...</p>
<p>If you&#x2019;re fortunate enough to be reading the Safari version of the book, you
have full-blown time-lapse animations just like in &#x201C;Streaming 102&#x201D;. For print,
Kindle, and other ebook versions, there are static images with a link to
animated versions on the web.</p>
<p>Bear with me here. Fine-grained emotional expressions via composite
punctuation (i.e., emoticons) are strictly forbidden in O&#x2019;Reilly publications &lt;
winky-smiley/&gt;.</p>
<p>And indeed, we did just that with the original triggers feature in Beam. In
retrospect, we went a bit overboard. Future iterations will be simpler and
easier to use, and in this book I focus only on the pieces that are likely to
remain in some form or another.</p>
<p>More accurately, the input to the function is really the state at time <em>P</em> of
everything upstream of the point in the pipeline where the watermark is being
observed: the input source, buffered data, data actively being processed, and
so on; but conceptually it&#x2019;s simpler to think of it as a mapping from
processing time to event time.</p>
<p>Note that I specifically chose to omit the value of 9 from the heuristic
watermark because it will help me to make some important points about late
data and watermark lag. In reality, a heuristic watermark might be just as
likely to omit some other value(s) instead, which in turn could have</p>
<p>1</p>
<p>2</p>
<p>3</p>
<p>4</p>
<p>5</p>
<p>significantly less drastic effect on the watermark. If winnowing late-arriving
data from the watermark is your goal (which is very valid in some cases, such
as abuse detection, for which you just want to see a significant majority of the
data as quickly as possible), you don&#x2019;t necessarily want a heuristic watermark
rather than a perfect watermark. What you really want is a percentile
watermark, which explicitly drops some percentile of late-arriving data from
its calculations. See Chapter 3.</p>
<p>Which isn&#x2019;t to say there aren&#x2019;t use cases that care primarily about
correctness and not so much about latency; in those cases, using an accurate
watermark as the sole driver of output from a pipeline is a reasonable
approach.</p>
<p>And, as we know from before, this assertion is either guaranteed, in the case
of a perfect watermark being used, or an educated guess, in the case of a
heuristic watermark.</p>
<p>You might note that there should logically be a fourth mode: discarding and
retracting. That mode isn&#x2019;t terribly useful in most cases, so I don&#x2019;t discuss it
further here.</p>
<p>In retrospect, it probably would have been clearer to choose a different set
of names that are more oriented toward the observed nature of data in the
materialized stream (e.g., &#x201C;output modes&#x201D;) rather than names describing the
state management semantics that yield those data. Perhaps: discarding mode
&#x2192; delta mode, accumulating mode &#x2192; value mode, accumulating and
retracting mode &#x2192; value and retraction mode? However, the
discarding/accumulating/accumulating and retracting names are enshrined in
the 1.x and 2.x lineages of the Beam Model, so I don&#x2019;t want to introduce
potential confusion in the book by deviating. Also, it&#x2019;s very likely
accumulating modes will blend into the background more with Beam 3.0 and
the introduction of sink triggers; more on this when we discuss SQL in
Chapter 8.</p>
<p>6</p>
<p>7</p>
<p>8</p>
<p>9</p>
<h1 id="chapter-3-watermarks">Chapter 3. Watermarks</h1>
<p>So far, we have been looking at stream processing from the perspective of the
pipeline author or data scientist. Chapter 2 introduced watermarks as part of
the answer to the fundamental questions of <em>where</em> in event-time processing is
taking place and <em>when</em> in processing time results are materialized. In this
chapter, we approach the same questions, but instead from the perspective of
the underlying mechanics of the stream processing system. Looking at these
mechanics will help us motivate, understand, and apply the concepts around
watermarks. We discuss how watermarks are created at the point of data
ingress, how they propagate through a data processing pipeline, and how they
affect output timestamps. We also demonstrate how watermarks preserve the
guarantees that are necessary for answering the questions of <em>where</em> in event-
time data are processed and <em>when</em> it is materialized, while dealing with
unbounded data.</p>
<h2 id="definition">Definition</h2>
<p>Consider any pipeline that ingests data and outputs results continuously. We
wish to solve the general problem of when it is safe to call an event-time
window closed, meaning that the window does not expect any more data. To
do so we would like to characterize the progress that the pipeline is making
relative to its unbounded input.</p>
<p>One naive approach for solving the event-time windowing problem would be
to simply base our event-time windows on the current processing time. As we
saw in Chapter 1, we quickly run into trouble&#x2014;data processing and transport
is not instantaneous, so processing and event times are almost never equal.
Any hiccup or spike in our pipeline might cause us to incorrectly assign
messages to windows. Ultimately, this strategy fails because we have no
robust way to make any guarantees about such windows.</p>
<p>Another intuitive, but ultimately incorrect, approach would be to consider the</p>
<p>rate of messages processed by the pipeline. Although this is an interesting
metric, the rate may vary arbitrarily with changes in input, variability of
expected results, resources available for processing, and so on. Even more
important, rate does not help answer the fundamental questions of
completeness. Specifically, rate does not tell us when we have seen all of the
messages for a particular time interval. In a real-world system, there will be
situations in which messages are not making progress through the system.
This could be the result of transient errors (such as crashes, network failures,
machine downtime), or the result of persistent errors such as application-level
failures that require changes to the application logic or other manual
intervention to resolve. Of course, if lots of failures are occurring, a rate-of-
processing metric might be a good proxy for detecting this. However a rate
metric could never tell us that a single message is failing to make progress
through our pipeline. Even a single such message, however, can arbitrarily
affect the correctness of the output results.</p>
<p>We require a more robust measure of progress. To arrive there, we make one
fundamental assumption about our streaming data: <em>each message has an
associated logical event timestamp</em>. This assumption is reasonable in the
context of continuously arriving unbounded data because this implies the
continuous generation of input data. In most cases, we can take the time of
the original event&#x2019;s occurrence as its logical event timestamp. With all input
messages containing an event timestamp, we can then examine the
distribution of such timestamps in any pipeline. Such a pipeline might be
distributed to process in parallel over many agents and consuming input
messages with no guarantee of ordering between individual shards. Thus, the
set of event timestamps for active in-flight messages in this pipeline will form
a distribution, as illustrated in Figure 3-1.</p>
<p>Messages are ingested by the pipeline, processed, and eventually marked
completed. Each message is either &#x201C;in-flight,&#x201D; meaning that it has been
received but not yet completed, or &#x201C;completed,&#x201D; meaning that no more
processing on behalf of this message is required. If we examine the
distribution of messages by event time, it will look something like Figure 3-1.
As time advances, more messages will be added to the &#x201C;in-flight&#x201D; distribution</p>
<p>on the right, and more of those messages from the &#x201C;in-flight&#x201D; part of the
distribution will be completed and moved into the &#x201C;completed&#x201D; distribution.</p>
<pre><code>Figure 3-1. Distribution of in-flight and completed message event times within a streaming
pipeline. New messages arrive as input and remain &#x201C;in-flight&#x201D; until processing for them
completes. The leftmost edge of the &#x201C;in-flight&#x201D; distribution corresponds to the oldest
unprocessed element at any given moment.
</code></pre><p>There is a key point on this distribution, located at the leftmost edge of the
&#x201C;in-flight&#x201D; distribution, corresponding to the oldest event timestamp of any
unprocessed message of our pipeline. We use this value to define the
watermark:</p>
<pre><code>00:00 / 00:00
</code></pre><pre><code>1
</code></pre><pre><code>The watermark is a monotonically increasing timestamp of the oldest
work not yet completed.
</code></pre><p>There are two fundamental properties that are provided by this definition that
make it useful:</p>
<p>Completeness</p>
<pre><code>If the watermark has advanced past some timestamp T , we are guaranteed
by its monotonic property that no more processing will occur for on-time
(nonlate data) events at or before T. Therefore, we can correctly emit any
aggregations at or before T. In other words, the watermark allows us to
know when it is correct to close a window.
</code></pre><p>Visibility</p>
<pre><code>If a message is stuck in our pipeline for any reason, the watermark cannot
advance. Furthermore, we will be able to find the source of the problem
by examining the message that is preventing the watermark from
advancing.
</code></pre><h2 id="source-watermark-creation">Source Watermark Creation</h2>
<p>Where do these watermarks come from? To establish a watermark for a data
source, we must assign a logical event timestamp to every message entering
the pipeline from that source. As Chapter 2 informs us, all watermark
creation falls into one of two broad categories: <em>perfect</em> or <em>heuristic</em>. To remind
ourselves about the difference between perfect and heuristic watermarks, let&#x2019;s
look at Figure 3-2, which presents the windowed summation example from
Chapter 2.</p>
<pre><code>1
</code></pre><p><em>Figure 3-2. Windowed summation with perfect (left) and heuristic (right) watermarks</em></p>
<pre><code>00:00 / 00:00
</code></pre><p>Notice that the distinguishing feature is that perfect watermarks ensure that
the watermark accounts for <em>all</em> data, whereas heuristic watermarks admit
some late-data elements.</p>
<p>After the watermark is created as either perfect or heuristic, watermarks
remain so throughout the rest of the pipeline. As to what makes watermark
creation perfect or heuristic, it depends a great deal on the nature of the
source that&#x2019;s being consumed. To see why, let&#x2019;s look at a few examples of
each type of watermark creation.</p>
<h2 id="perfect-watermark-creation">Perfect Watermark Creation</h2>
<p>Perfect watermark creation assigns timestamps to incoming messages in such
a way that the resulting watermark is a <em>strict guarantee</em> that no data with
event times less than the watermark will ever be seen again from this source.
Pipelines using perfect watermark creation never have to deal with late data;
that is, data that arrive after the watermark has advanced past the event times
of newly arriving messages. However, perfect watermark creation requires
perfect knowledge of the input, and thus is impractical for many real-world
distributed input sources. Here are a couple of examples of use cases that can
create perfect watermarks:</p>
<p>Ingress timestamping</p>
<pre><code>A source that assigns ingress times as the event times for data entering the
system can create a perfect watermark. In this case, the source watermark
simply tracks the current processing time as observed by the pipeline.
This is essentially the method that nearly all streaming systems
supporting windowing prior to 2016 used.
Because event times are assigned from a single, monotonically increasing
source (actual processing time), the system thus has perfect knowledge
about which timestamps will come next in the stream of data. As a result,
event-time progress and windowing semantics become vastly easier to
reason about. The downside, of course, is that the watermark has no
correlation to the event times of the data themselves; those event times
were effectively discarded, and the watermark instead merely tracks the
</code></pre><pre><code>progress of data relative to its arrival in the system.
</code></pre><p>Static sets of time-ordered logs</p>
<pre><code>A statically sized input source of time-ordered logs (e.g., an Apache
Kafka topic with a static set of partitions, where each partition of the
source contains monotonically increasing event times) would be
relatively straightforward source atop which to create a perfect
watermark. To do so, the source would simply track the minimum event
time of unprocessed data across the known and static set of source
partitions (i.e., the minimum of the event times of the most recently read
record in each of the partitions).
Similar to the aforementioned ingress timestamps, the system has perfect
knowledge about which timestamps will come next, thanks to the fact that
event times across the static set of partitions are known to increase
monotonically. This is effectively a form of bounded out-of-order
processing; the amount of disorder across the known set of partitions is
bounded by the minimum observed event time among those partitions.
Typically, the only way you can guarantee monotonically increasing
timestamps within partitions is if the timestamps within those partitions
are assigned as data are written to it; for example, by web frontends
logging events directly into Kafka. Though still a limited use case, this is
definitely a much more useful one than ingress timestamping upon arrival
at the data processing system because the watermark tracks meaningful
event times of the underlying data.
</code></pre><h2 id="heuristic-watermark-creation">Heuristic Watermark Creation</h2>
<p>Heuristic watermark creation, on the other hand, creates a watermark that is
merely an <em>estimate</em> that no data with event times less than the watermark will
ever be seen again. Pipelines using heuristic watermark creation might need
to deal with some amount of <em>late data</em>. Late data is any data that arrives after
the watermark has advanced past the event time of this data. Late data is only
possible with heuristic watermark creation. If the heuristic is a reasonably</p>
<pre><code>2
</code></pre><p>good one, the amount of late data might be very small, and the watermark
remains useful as a completion estimate. The system still needs to provide a
way for the user to cope with late data if it&#x2019;s to support use cases requiring
correctness (e.g., things like billing).</p>
<p>For many real-world, distributed input sources, it&#x2019;s computationally or
operationally impractical to construct a perfect watermark, but still possible
to build a highly accurate heuristic watermark by taking advantage of
structural features of the input data source. Following are two example for
which heuristic watermarks (of varying quality) are possible:</p>
<p>Dynamic sets of time-ordered logs</p>
<pre><code>Consider a dynamic set of structured log files (each individual file
containing records with monotonically increasing event times relative to
other records in the same file but with no fixed relationship of event times
between files), where the full set of expected log files (i.e., partitions, in
Kafka parlance) is not known at runtime. Such inputs are often found in
global-scale services constructed and managed by a number of
independent teams. In such a use case, creating a perfect watermark over
the input is intractable, but creating an accurate heuristic watermark is
quite possible.
By tracking the minimum event times of unprocessed data in the existing
set of log files, monitoring growth rates, and utilizing external
information like network topology and bandwidth availability, you can
create a remarkably accurate watermark, even given the lack of perfect
knowledge of all the inputs. This type of input source is one of the most
common types of unbounded datasets found at Google, so we have
extensive experience with creating and analyzing watermark quality for
such scenarios and have seen them used to good effect across a number of
use cases.
</code></pre><p>Google Cloud Pub/Sub</p>
<pre><code>Cloud Pub/Sub is an interesting use case. Pub/Sub currently makes no
guarantees on in-order delivery; even if a single publisher publishes two
messages in order, there&#x2019;s a chance (usually small) that they might be
</code></pre><pre><code>delivered out of order (this is due to the dynamic nature of the underlying
architecture, which allows for transparent scaling up to very high levels
of throughput with zero user intervention). As a result, there&#x2019;s no way to
guarantee a perfect watermark for Cloud Pub/Sub. The Cloud Dataflow
team has, however, built a reasonably accurate heuristic watermark by
taking advantage of what knowledge is available about the data in Cloud
Pub/Sub. The implementation of this heuristic is discussed at length as a
case study later in this chapter.
</code></pre><p>Consider an example where users play a mobile game, and their scores are
sent to our pipeline for processing: you can generally assume that for any
source utilizing mobile devices for input it will be generally impossible to
provide a perfect watermark. Due to the problem of devices that go offline for
extended periods of time, there&#x2019;s just no way to provide any sort of
reasonable estimate of absolute completeness for such a data source. You
can, however, imagine building a watermark that accurately tracks input
completeness for devices that are currently online, similar to the Google
Pub/Sub watermark described a moment ago. Users who are actively online
are likely the most relevant subset of users from the perspective of providing
low-latency results anyway, so this often isn&#x2019;t as much of a shortcoming as
you might initially think.</p>
<p>With heuristic watermark creation, broadly speaking, the more that is known
about the source, the better the heuristic, and the fewer late data items will be
seen. There is no one-size-fits-all solution, given that the types of sources,
distributions of events, and usage patterns will vary greatly. But in either case
(perfect or heuristic), after a watermark is created at the input source, the
system can propagate the watermark through the pipeline perfectly. This
means perfect watermarks will remain perfect downstream, and heuristic
watermarks will remain strictly as heuristic as they were when established.
This is the benefit of the watermark approach: you can reduce the complexity
of tracking completeness in a pipeline entirely to the problem of creating a
watermark at the source.</p>
<h2 id="watermark-propagation">Watermark Propagation</h2>
<p>So far, we have considered only the watermark for the inputs within the
context of a single operation or stage. However, most real-world pipelines
consist of multiple stages. Understanding how watermarks propagate across
independent stages is important in understanding how they affect the pipeline
as a whole and the observed latency of its results.</p>
<h3 id="pipeline-stages">PIPELINE STAGES</h3>
<pre><code>Different stages are typically necessary every time your pipeline groups
data together by some new dimension. For example, if you had a pipeline
that consumed raw data, computed some per-user aggregates, and then
used those per-user aggregates to compute some per-team aggregates,
you&#x2019;d likely end up with a three-stage pipeline:
</code></pre><pre><code>One consuming the raw, ungrouped data
</code></pre><pre><code>One grouping the data by user and computing per-user
aggregates
One grouping the data by team and computing per-team
aggregates
</code></pre><pre><code>We learn more about the effects of grouping on pipeline shapes in
Chapter 6.
</code></pre><p>Watermarks are created at input sources, as discussed in the preceding
section. They then conceptually flow through the system as data progress
through it. You can track watermarks at varying levels of granularity. For
pipelines comprising multiple distinct stages, each stage likely tracks its own
watermark, whose value is a function of all the inputs and stages that come
before it. Therefore, stages that come later in the pipeline will have
watermarks that are further in the past (because they&#x2019;ve seen less of the
overall input).</p>
<p>We can define watermarks at the boundaries of any single operation, or stage,</p>
<pre><code>3
</code></pre><p>in the pipeline. This is useful not only in understanding the relative progress
that each stage in the pipeline is making, but for dispatching timely results
independently and as soon as possible for each individual stage. We give the
following definitions for the watermarks at the boundaries of stages:</p>
<pre><code>An input watermark , which captures the progress of everything
upstream of that stage (i.e., how complete the input is for that stage).
For sources, the input watermark is a source-specific function
creating the watermark for the input data. For nonsource stages, the
input watermark is defined as the minimum of the output
watermarks of all shards/partitions/instances of all of its upstream
sources and stages.
An output watermark , which captures the progress of the stage itself,
and is essentially defined as the minimum of the stage&#x2019;s input
watermark and the event times of all nonlate data active messages
within the stage. Exactly what &#x201C;active&#x201D; encompasses is somewhat
dependent upon the operations a given stage actually performs, and
the implementation of the stream processing system. It typically
includes data buffered for aggregation but not yet materialized
downstream, pending output data in flight to downstream stages, and
so on.
</code></pre><p>One nice feature of defining an input and output watermark for a specific
stage is that we can use these to calculate the amount of event-time latency
introduced by a stage. Subtracting the value of a stage&#x2019;s output watermark
from the value of its input watermark gives the amount of event-time latency
or <em>lag</em> introduced by the stage. This lag is the notion of how far delayed
behind real time the output of each stage will be. As an example, a stage
performing 10-second windowed aggregations will have a lag of 10 seconds
or more, meaning that the output of the stage will be at least that much
delayed behind the input and real time. Definitions of input and output
watermarks provide a recursive relationship of watermarks throughout a
pipeline. Each subsequent stage in a pipeline delays the watermark as
necessary, based on event-time lag of the stage.</p>
<p>Processing within each stage is also not monolithic. We can segment the
processing within one stage into a flow with several conceptual components,
each of which contributes to the output watermark. As mentioned previously,
the exact nature of these components depends on the operations the stage
performs and the implementation of the system. Conceptually, each such
component serves as a buffer where active messages can reside until some
operation has completed. For example, as data arrives, it is buffered for
processing. Processing might then write the data to state for later delayed
aggregation. Delayed aggregation, when triggered, might write the results to
an output buffer awaiting consumption from a downstream stage, as shown in
Figure 3-3.</p>
<pre><code>Figure 3-3. Example system components of a streaming system stage, containing buffers of
in-flight data. Each will have associated watermark tracking, and the overall output
watermark of the stage will be the minimum of the watermarks across all such buffers.
</code></pre><p>We can track each such buffer with its own watermark. The minimum of the
watermarks across the buffers of each stage forms the output watermark of
the stage. Thus the output watermark could be the minimum of the following:</p>
<pre><code>Per-source watermark&#x2014;for each sending stage.
Per-external input watermark&#x2014;for sources external to the pipeline
</code></pre><pre><code>Per-state component watermark&#x2014;for each type of state that can be
written
</code></pre><pre><code>Per-output buffer watermark&#x2014;for each receiving stage
</code></pre><p>Making watermarks available at this level of granularity also provides better
visibility into the behavior of the system. The watermarks track locations of
messages across various buffers in the system, allowing for easier diagnosis
of stuckness.</p>
<h2 id="understanding-watermark-propagation">Understanding Watermark Propagation</h2>
<p>To get a better sense for the relationship between input and output
watermarks and how they affect watermark propagation, let&#x2019;s look at an
example. Let&#x2019;s consider gaming scores, but instead of computing sums of
team scores, we&#x2019;re going to take a stab at measuring user engagement levels.
We&#x2019;ll do this by first calculating per-user session lengths, under the
assumption that the amount of time a user stays engaged with the game is a
reasonable proxy for how much they&#x2019;re enjoying it. After answering our four
questions once to calculate sessions lengths, we&#x2019;ll then answer them a second
time to calculate average session lengths within fixed periods of time.</p>
<p>To make our example even more interesting, lets say that we are working
with two datasets, one for Mobile Scores and one for Console Scores. We
would like to perform identical score calculations via integer summation in
parallel over these two independant datasets. One pipeline is calculating
scores for users playing on mobile devices, whereas the other is for users
playing on home gaming consoles, perhaps due to different data collection
strategies employed for the different platforms. The important point is that
these two stages are performing the same operation but over different data,
and thus with very different output watermarks.</p>
<p>To begin, let&#x2019;s take a look at Example 3-1 to see what the abbreviated code
for what the first section of this pipeline might be like.</p>
<p><em>Example 3-1. Calculating session lengths</em></p>
<p>PCollection<double> mobileSessions = IO.read(new MobileInputSource())</double></p>
<p>.apply(Window.into(Sessions.withGapDuration(Duration.standardMinutes(1)))
.triggering(AtWatermark())</p>
<p>.discardingFiredPanes())
.apply(CalculateWindowLength());</p>
<p>PCollection<double> consoleSessions = IO.read(new ConsoleInputSource())</double></p>
<p>.apply(Window.into(Sessions.withGapDuration(Duration.standardMinutes(1)))
.triggering(AtWatermark())
.discardingFiredPanes())
.apply(CalculateWindowLength());</p>
<p>Here, we read in each of our inputs independently, and whereas previously
we were keying our collections by team, in this example we key by user.
After that, for the first stage of each pipeline, we window into sessions and
then call a custom PTransform named CalculateWindowLength. This
PTransform simply groups by key (i.e., User) and then computes the per-
user session length by treating the size of the current window as the value for
that window. In this case, we&#x2019;re fine with the default trigger (AtWatermark)
and accumulation mode (discardingFiredPanes) settings, but I&#x2019;ve listed
them explicitly for completeness. The output for each pipeline for two
particular users might look something like Figure 3-4.</p>
<pre><code>Figure 3-4. Per-user session lengths across two different input pipelines
</code></pre><p>Because we need to track data across multiple stages, we track everything
related to Mobile Scores in red, everything related to Console Scores in blue,
while the watermark and output for Average Session Lengths in Figure 3-5
are yellow.</p>
<p>We have answered the four questions of <em>what</em> , <em>where</em> , <em>when</em> , and <em>how</em> to
compute individual session lengths. Next we&#x2019;ll answer them a second time to
transform those session lengths into global session-length averages within</p>
<pre><code>00:00 / 00:00
</code></pre><p>fixed windows of time. This requires us to first flatten our two data sources
into one, and then re-window into fixed windows; we&#x2019;ve already captured the
important essence of the session in the session-length value we computed,
and we now want to compute a global average of those sessions within
consistent windows of time over the course of the day. Example 3-2 shows
the code for this.</p>
<p><em>Example 3-2. Calculating session lengths</em></p>
<p>PCollection<double> mobileSessions = IO.read(new MobileInputSource())</double></p>
<p>.apply(Window.into(Sessions.withGapDuration(Duration.standardMinutes(1)))
.triggering(AtWatermark())
.discardingFiredPanes())
.apply(CalculateWindowLength());</p>
<p>PCollection<double> consoleSessions = IO.read(new ConsoleInputSource())</double></p>
<p>.apply(Window.into(Sessions.withGapDuration(Duration.standardMinutes(1)))
.triggering(AtWatermark())
.discardingFiredPanes())
.apply(CalculateWindowLength());</p>
<p>PCollection<float> averageSessionLengths = PCollectionList
.of(mobileSessions).and(consoleSessions)
.apply(Flatten.pCollections())
.apply(Window.into(FixedWindows.of(Duration.standardMinutes(2)))
.triggering(AtWatermark())
.apply(Mean.globally());</float></p>
<p>If we were to see this pipeline in action, it would look something like
Figure 3-5. As before, the two input pipelines are computing individual
session lengths for mobile and console players. Those session lengths then
feed into the second stage of the pipeline, where global session-length
averages are computed in fixed windows.</p>
<pre><code>Figure 3-5. Average session lengths of mobile and console gaming sessions
</code></pre><p>Let&#x2019;s walk through some of this example, given that there&#x2019;s a lot going on.
The two important points here are:</p>
<pre><code>The output watermark for each of the Mobile Sessions and Console
Sessions stages is at least as old as the corresponding input
watermark of each, and in reality a little bit older. This is because in
a real system computing answers takes time, and we don&#x2019;t allow the
output watermark to advance until processing for a given input has
completed.
The input watermark for the Average Session Lengths stage is the
minimum of the output watermarks for the two stages directly
upstream.
</code></pre><p>The result is that the downstream input watermark is an alias for the
minimum composition of the upstream output watermarks. Note that this
matches the definitions for those two types of watermarks earlier in the
chapter. Also notice how watermarks further downstream are further in the
past, capturing the intuitive notion that upstream stages are going to be
further ahead in time than the stages that follow them.</p>
<p>One observation worth making here is just how cleanly we were able to ask
the questions again in Example 3-1 to substantially alter the results of the
pipeline. Whereas before we simply computed per-user session lengths, we
now compute two-minute global session-length averages. This provides a
much more insightful look into the overall behaviors of the users playing our
games and gives you a tiny glimpse of the difference between simple data
transformations and real data science.</p>
<pre><code>00:00 / 00:00
</code></pre><p>Even better, now that we understand the basics of how this pipeline operates,
we can look more closely at one of the more subtle issues related to asking
the four questions over again: <em>output timestamps</em>.</p>
<h2 id="watermark-propagation-and-output-timestamps">Watermark Propagation and Output Timestamps</h2>
<p>In Figure 3-5, I glossed over some of the details of output timestamps. But if
you look closely at the second stage in the diagram, you can see that each of
the outputs from the first stage was assigned a timestamp that matched the
end of its window. Although that&#x2019;s a fairly natural choice for output
timestamps, it&#x2019;s not the only valid choice. As you know from earlier in this
chapter, watermarks are never allowed to move backward. Given that
restriction, you can infer that the range of valid timestamps for a given
window begins with the timestamp of the earliest nonlate record in the
window (because only nonlate records are guaranteed to hold a watermark
up) and extends all the way to positive infinity. That&#x2019;s quite a lot of options.
In practice, however, there tend to be only a few choices that make sense in
most circumstances:</p>
<p>End of the window</p>
<pre><code>Using the end of the window is the only safe choice if you want the
output timestamp to be representative of the window bounds. As we&#x2019;ll see
in a moment, it also allows the smoothest watermark progression out of
all of the options.
</code></pre><p>Timestamp of first nonlate element</p>
<pre><code>Using the timestamp of the first nonlate element is a good choice when
you want to keep your watermarks as conservative as possible. The trade-
off, however, is that watermark progress will likely be more hindered, as
we&#x2019;ll also see shortly.
</code></pre><p>Timestamp of a specific element</p>
<pre><code>For certain use cases, the timestamp of some other arbitrary (from the
system&#x2019;s perspective) element is the right choice. Imagine a use case in
which you&#x2019;re joining a stream of queries to a stream of clicks on results
</code></pre><pre><code>4
</code></pre><pre><code>for that query. After performing the join, some systems will find the
timestamp of the query to be more useful; others will prefer the
timestamp of the click. Any such timestamp is valid from a watermark
correctness perspective, as long as it corresponded to an element that did
not arrive late.
</code></pre><p>Having thought a bit about some alternate options for output timestamps,
let&#x2019;s look at what effects the choice of output timestamp can have on the
overall pipeline. To make the changes as dramatic as possible, in Example 3-
3 and Figure 3-6, we&#x2019;ll switch to using the earliest timestamp possible for the
window: the timestamp of the first nonlate element as the timestamp for the
window.</p>
<p><em>Example 3-3. Average session lengths pipeline, that output timestamps for
session windows set at earliest element</em></p>
<p>PCollection<double> mobileSessions = IO.read(new MobileInputSource())</double></p>
<p>.apply(Window.into(Sessions.withGapDuration(Duration.standardMinutes(1)))
.triggering(AtWatermark())
.withTimestampCombiner(EARLIEST)
.discardingFiredPanes())
.apply(CalculateWindowLength());</p>
<p>PCollection<double> consoleSessions = IO.read(new ConsoleInputSource())</double></p>
<p>.apply(Window.into(Sessions.withGapDuration(Duration.standardMinutes(1)))
.triggering(AtWatermark())
.withTimestampCombiner(EARLIEST)
.discardingFiredPanes())
.apply(CalculateWindowLength());</p>
<p>PCollection<float> averageSessionLengths = PCollectionList
.of(mobileSessions).and(consoleSessions)
.apply(Flatten.pCollections())
.apply(Window.into(FixedWindows.of(Duration.standardMinutes(2)))
.triggering(AtWatermark())
.apply(Mean.globally());</float></p>
<pre><code>Figure 3-6. Average session lengths for sessions that are output at the timestamp of the
earliest element
</code></pre><p>To help call out the effect of the output timestamp choice, look at the dashed
lines in the first stages showing what the output watermark for each stage is
being held to. The output watermark is delayed by our choice of timestamp,
as compared to Figures 3-7 and 3-8, in which the output timestamp was
chosen to be the end of the window. You can see from this diagram that the
input watermark of the second stage is thus subsequently also delayed.</p>
<pre><code>Figure 3-7. Comparison of watermarks and results with different choice of window outout
</code></pre><pre><code>00:00 / 00:00
</code></pre><pre><code>timestamps. The watermarks in this figure correspond to output timestamps at the end of
the session windows (i.e., Figure 3-5).
</code></pre><pre><code>Figure 3-8. In this figure, the watermarks are at the beginning of the session windows (i.e.,
Figure 3-6). We can see that the watermark line in this figure is more delayed, and the
resulting average session lengths are different.
</code></pre><p>As far as differences in this version compared to Figure 3-7, two are worth
noting:</p>
<p>Watermark delay</p>
<pre><code>Compared to Figure 3-5, the watermark proceeds much more slowly in
Figure 3-6. This is because the output watermark for the first stage is held
back to the timestamp of the first element in every window until the input
for that window becomes complete. Only after a given window has been
materialized is the output watermark (and thus the downstream input
watermark) allowed to advance.
</code></pre><p>Semantic differences</p>
<pre><code>Because the session timestamps are now assigned to match the earliest
nonlate element in the session, the individual sessions often end up in
different fixed window buckets when we then calculate the session-length
averages in the next stage. There&#x2019;s nothing inherently right or wrong
about either of the two options we&#x2019;ve seen so far; they&#x2019;re just different.
But it&#x2019;s important to understand that they will be different as well as have
an intuition for the way in which they&#x2019;ll be different so that you can make
the correct choice for your specific use case when the time comes.
</code></pre><h2 id="the-tricky-case-of-overlapping-windows">The Tricky Case of Overlapping Windows</h2>
<p>One additional subtle but important issue regarding output timestamps is how
to handle sliding windows. The naive approach of setting the output
timestamp to the earliest element can very easily lead to delays downstream
due to watermarks being (correctly) held back. To see why, consider an
example pipeline with two stages, each using the same type of sliding
windows. Suppose that each element ends up in three successive windows.
As the input watermark advances, the desired semantics for sliding windows
in this case would be as follows:</p>
<pre><code>The first window completes in the first stage and is emitted
downstream.
The first window then completes in the second stage and can also be
emitted downstream.
</code></pre><pre><code>Some time later, the second window completes in the first stage...
and so on.
</code></pre><p>However, if output timestamps are chosen to be the timestamp of the first
nonlate element in the pane, what actually happens is the following:</p>
<pre><code>The first window completes in the first stage and is emitted
downstream.
The first window in the second stage remains unable to complete
</code></pre><pre><code>because its input watermark is being held up by the output
watermark of the second and third windows upstream. Those
watermarks are rightly being held back because the earliest element
timestamp is being used as the output timestamp for those windows.
</code></pre><pre><code>The second window completes in the first stage and is emitted
downstream.
The first and second windows in the second stage remain unable to
complete, held up by the third window upstream.
The third window completes in the first stage and is emitted
downstream.
</code></pre><pre><code>The first, second, and third windows in the second stage are now all
able to complete, finally emitting all three in one swoop.
</code></pre><p>Although the results of this windowing are correct, this leads to the results
being materialized in an unnecessarily delayed way. Because of this, Beam
has special logic for overlapping windows that ensures the output timestamp
for window <em>N</em> +1 is always greater than the end of window <em>N</em>.</p>
<h2 id="percentile-watermarks">Percentile Watermarks</h2>
<p>So far, we have concerned ourselves with watermarks as measured by the
minimum event time of active messages in a stage. Tracking the minimum
allows the system to know when all earlier timestamps have been accounted
for. On the other hand, we could consider the entire distribution of event
timestamps for active messages and make use of it to create finer-grained
triggering conditions.</p>
<p>Instead of considering the minimum point of the distribution, we could take
any percentile of the distribution and say that we are guaranteed to have
processed this percentage of all events with earlier timestamps.</p>
<p>What is the advantage of this scheme? If for the business logic &#x201C;mostly&#x201D;
correct is sufficient, percentile watermarks provide a mechanism by which</p>
<pre><code>5
</code></pre><p>the watermark can advance more quickly and more smoothly than if we were
tracking the minimum event time by discarding outliers in the long tail of the
distribution from the watermark. Figure 3-9 shows a compact distribution of
event times where the 90 percentile watermark is close to the 100
percentile. Figure 3-10 demonstrates a case where the outlier is further
behind, so the 90 percentile watermark is significantly ahead of the 100
percentile. By discarding the outlier data from the watermark, the percentile
watermark can still keep track of the bulk of the distribution without being
delayed by the outliers.</p>
<pre><code>Figure 3-9. Normal-looking watermark histogram
</code></pre><pre><code>Figure 3-10. Watermark histogram with outliers
</code></pre><pre><code>th th
</code></pre><pre><code>th th
</code></pre><p>Figure 3-11 shows an example of percentile watermarks used to draw
window boundaries for two-minute fixed windows. We can draw early
boundaries based on the percentile of timestamps of arrived data as tracked
by the percentile watermark.</p>
<pre><code>Figure 3-11. Effects of varying watermark percentiles. As the percentile increases, more
events are included in the window: however, the processing time delay to materialize the
window also increases.
</code></pre><p>Figure 3-11 shows the 33 percentile, 66 percentile, and 100 percentile
(full) watermark, tracking the respective timestamp percentiles in the data
distribution. As expected, these allow boundaries to be drawn earlier than
tracking the full 100 percentile watermark. Notice that the 33 and 66
percentile watermarks each allow earlier triggering of windows but with the
trade-off of marking more data as late. For example, for the first window,
[12:00, 12:02), a window closed based on the 33 percentile watermark
would include only four events and materialize the result at 12:06 processing
time. If we use the 66 percentile watermark, the same event-time window
would include seven events, and materialize at 12:07 processing time. Using
the 100 percentile watermark includes all ten events and delays
materializing the results until 12:08 processing time. Thus, percentile
watermarks provide a way to tune the trade-off between latency of
materializing results and precision of the results.</p>
<h2 id="processing-time-watermarks">Processing-Time Watermarks</h2>
<p>Until now, we have been looking at watermarks as they relate to the data
flowing through our system. We have seen how looking at the watermark can
help us identify the overall delay between our oldest data and real time.</p>
<pre><code>00:00 / 00:00
</code></pre><pre><code>rd th th
</code></pre><pre><code>th rd th
</code></pre><pre><code>rd
</code></pre><pre><code>th
</code></pre><pre><code>th
</code></pre><p>However, this is not enough to distinguish between old data and a delayed
system. In other words, by only examining the event-time watermark as we
have defined it up until now, we cannot distinguish between a system that is
processing data from an hour ago quickly and without delay, and a system
that is attempting to process real-time data and has been delayed for an hour
while doing so.</p>
<p>To make this distinction, we need something more: processing-time
watermarks. We have already seen that there are two time domains in a
streaming system: processing time and event time. Until now, we have
defined the watermark entirely in the event-time domain, as a function of
timestamps of the data flowing through the system. This is an event-time
watermark. We will now apply the same model to the processing-time
domain to define a processing-time watermark.</p>
<p>Our stream processing system is constantly performing operations such as
shuffling messages between stages, reading or writing messages to persistent
state, or triggering delayed aggregations based on watermark progress. All of
these operations are performed in response to previous operations done at the
current or upstream stage of the pipeline. Thus, just as data elements &#x201C;flow&#x201D;
through the system, a cascade of operations involved in processing these
elements also &#x201C;flows&#x201D; through the system.</p>
<p>We define the processing-time watermark in the exact same way as we have
defined the event-time watermark, except instead of using the event-time
timestamp of oldest work not yet completed, we use the processing-time
timestamp of the oldest operation not yet completed. An example of delay to
the processing-time watermark could be a stuck message delivery from one
stage to another, a stuck I/O call to read state or external data, or an exception
while processing that prevents processing from completing.</p>
<p>The processing-time watermark, therefore, provides a notion of processing
delay separate from the data delay. To understand the value of this
distinction, consider the graph in Figure 3-12 where we look at the event-time
watermark delay.</p>
<p>We see that the data delay is monotonically increasing, but there is not</p>
<p>enough information to distinguish between the cases of a stuck system and
stuck data. Only by looking at the processing-time watermark, shown in
Figure 3-13, can we distinguish the cases.</p>
<pre><code>Figure 3-12. Event-time watermark increasing. It is not possible to know from this
information whether this is due to data buffering or system processing delay.
</code></pre><pre><code>Figure 3-13. Processing-time watermark also increasing. This indicates that the system
processing is delayed.
</code></pre><p>In the first case (Figure 3-12), when we examine the processing-time
watermark delay we see that it too is increasing. This tells us that an
operation in our system is stuck, and the stuckness is also causing the data
delay to fall behind. Some real-world examples of situations in which this
might occur are when there is a network issue preventing message delivery
between stages of a pipeline or if a failure has occurred and is being retried.
In general, a growing processing-time watermark indicates a problem that is
preventing operations from completing that are necessary to the system&#x2019;s
function, and often involves user or administrator intervention to resolve.</p>
<p>In this second case, as seen in Figure 3-14, the processing-time watermark</p>
<p>delay is small. This tells us that there are no stuck operations. The event-time
watermark delay is still increasing, which indicates that we have some
buffered state that we are waiting to drain. This is possible, for example, if
we are buffering some state while waiting for a window boundary to emit an
aggregation, and corresponds to a normal operation of the pipeline, as in
Figure 3-15.</p>
<pre><code>Figure 3-14. Event-time watermark delay increasing, processing-time watermark stable.
This is an indication that data are buffered in the system and waiting to be processed,
rather than an indication that a system operation is preventing data processing from
completing.
</code></pre><pre><code>Figure 3-15. Watermark delay for fixed windows. The event-time watermark delay
increases as elements are buffered for each window, and decreases as each window&#x2019;s
aggregate is emitted via an on-time trigger, whereas the processing-time watermark simply
tracks system-level delays (which remain relatively steady in a healthy pipeline).
</code></pre><p>Therefore, the processing-time watermark is a useful tool in distinguishing
system latency from data latency. In addition to visibility, we can use the
processing-time watermark at the system-implementation level for tasks such
as garbage collection of temporary state (Reuven talks more about an</p>
<p>example of this in Chapter 5).</p>
<h2 id="case-studies">Case Studies</h2>
<p>Now that we&#x2019;ve laid the groundwork for how watermarks ought to behave,
it&#x2019;s time to take a look at some real systems to understand how different
mechanisms of the watermark are implemented. We hope that these shed
some light on the trade-offs that are possible between latency and correctness
as well as scalability and availability for watermarks in real-world systems.</p>
<h2 id="case-study-watermarks-in-google-cloud">Case Study: Watermarks in Google Cloud</h2>
<h2 id="dataflow">Dataflow</h2>
<p>There are many possible approaches to implementing watermarks in a stream
processing system. Here, we present a quick survey of the implementation in
Google Cloud Dataflow, a fully managed service for executing Apache Beam
pipelines. Dataflow includes SDKs for defining data processing workflows,
and a Cloud Platform managed service to run those workflows on Google
Cloud Platform resources.</p>
<p>Dataflow stripes (shards) each of the data processing steps in its data
processing graph across multiple physical workers by splitting the available
keyspace of each worker into key ranges and assigning each range to a</p>
<p>worker. Whenever a GroupByKey operation with distinct keys is encountered,
data must be shuffled to corresponding keys.</p>
<p>Figure 3-16 depicts a logical representation of the processing graph with a
GroupByKey.</p>
<pre><code>Figure 3-16. A GroupByKey step consumes data from another DoFn. This means that there
is a data shuffle between the keys of the first step and the keys of the second step.
</code></pre><p>Whereas the physical assignment of key ranges to workers might look
Figure 3-17.</p>
<p><em>Figure 3-17. Key ranges of both steps are assigned (striped) across the available workers.</em></p>
<p>In the watermark propagation section, we discussed that the watermark is
maintained for multiple subcomponents of each step. Dataflow keeps track of
the per-range watermarks of each of these components. Watermark
aggregation then involves computing the minimum of each watermark across
all ranges, ensuring that the following guarantees are met:</p>
<pre><code>All ranges must be reporting a watermark. If a watermark is not
present for a range, we cannot advance the watermark, because a
range not reporting must be treated as unknown.
Ensure that the watermark is monotonically increasing. Because late
data is possible, we must not update the watermark if it would cause
the watermark to move backward.
</code></pre><p>Google Cloud Dataflow performs aggregation via a centralized aggregator
agent. We can shard this agent for efficiency. From a correctness standpoint,
the watermark aggregator serves as a &#x201C;single source of truth&#x201D; about the
watermark.</p>
<p>Ensuring correctness in distributed watermark aggregation poses certain
challenges. It is paramount that watermarks are not advanced prematurely
because advancing the watermark prematurely will turn on-time data into late
data. Specifically, as physical assignments are actuated to workers, the
workers maintain leases on the persistent state attached to the key ranges,
ensuring that only a single worker may mutate the persistent state for a key.
To guarantee watermark correctness, we must ensure that each watermark
update from a worker process is admitted into the aggregate only if the
worker process still maintains a lease on its persistent state; therefore, the
watermark update protocol must take state ownership lease validation into
account.</p>
<h2 id="case-study-watermarks-in-apache-flink">Case Study: Watermarks in Apache Flink</h2>
<p>Apache Flink is an open source stream processing framework for distributed,
high-performing, always-available, and accurate data streaming applications.
It is possible to run Beam programs using a Flink runner. In doing so, Beam</p>
<p>relies on the implementation of stream processing concepts such as
watermarks within Flink. Unlike Google Cloud Dataflow, which implements
watermark aggregation via a centralized watermark aggregator agent, Flink
performs watermark tracking and aggregation in-band.</p>
<p>To understand how this works, let&#x2019;s look at a Flink pipeline, as shown in
Figure 3-18.</p>
<pre><code>Figure 3-18. A Flink pipeline with two sources and event-time watermarks propagating in-
band
</code></pre><p>In this pipeline data is generated at two sources. These sources also both
generate watermark &#x201C;checkpoints&#x201D; that are sent synchronously in-band with
the data stream. This means that when a watermark checkpoint from source A
for timestamp &#x201C;53&#x201D; is emitted, it guarantees that no nonlate data messages
will be emitted from source A with timestamp behind &#x201C;53&#x201D;. The downstream
&#x201C;keyBy&#x201D; operators consume the input data and the watermark checkpoints.
As new watermark checkpoints are consumed, the downstream operators&#x2019;
view of the watermark is advanced, and a new watermark checkpoint for
downstream operators can be emitted.</p>
<p>This choice to send watermark checkpoints in-band with the data stream
differs from the Cloud Dataflow approach that relies on central aggregation
and leads to a few interesting trade-offs.</p>
<pre><code>6
</code></pre><p>Following are some advantages of in-band watermarks:</p>
<p>Reduced watermark propagation latency, and very low-latency watermarks</p>
<pre><code>Because it is not necessary to have watermark data traverse multiple hops
and await central aggregation, it is possible to achieve very low latency
more easily with the in-band approach.
</code></pre><p>No single point of failure for watermark aggregation</p>
<pre><code>Unavailability in the central watermark aggregation agent will lead to a
delay in watermarks across the entire pipeline. With the in-band
approach, unavailability of part of the pipeline cannot cause watermark
delay to the entire pipeline.
</code></pre><p>Inherent scalability</p>
<pre><code>Although Cloud Dataflow scales well in practice, more complexity is
needed to achieve scalability with a centralized watermark aggregation
service versus implicit scalability with in-band watermarks.
</code></pre><p>Here are some advantages of out-of-band watermark aggregation:</p>
<p>Single source of &#x201C;truth&#x201D;</p>
<pre><code>For debuggability, monitoring, and other applications such as throttling
inputs based on pipeline progress, it is advantageous to have a service that
can vend the values of watermarks rather than having watermarks implicit
in the streams, with each component of the system having its own partial
view.
</code></pre><p>Source watermark creation</p>
<pre><code>Some source watermarks require global information. For example,
sources might be temprarily idle, have low data rates, or require out-of-
band information about the source or other system components to
generate the watermarks. This is easier to achieve in a central service. For
an example see the case study that follows on source watermarks for
Google Cloud Pub/Sub.
</code></pre><h2 id="case-study-source-watermarks-for-google-cloud">Case Study: Source Watermarks for Google Cloud</h2>
<h2 id="pubsub">Pub/Sub</h2>
<p>Google Cloud Pub/Sub is a fully managed real-time messaging service that
allows you to send and receive messages between independent applications.
Here, we discuss how to create a reasonable heuristic watermark for data sent
into a pipeline via Cloud Pub/Sub.</p>
<p>First, we need to describe a little about how Pub/Sub works. Messages are
published on Pub/Sub <em>topics</em>. A particular topic can be subscribed to by any
number of Pub/Sub <em>subscriptions</em>. The same messages are delivered on all
subscriptions subscribed to a given topic. The method of delivery is for
clients to <em>pull</em> messages off the subscription, and to ack the receipt of
particular messages via provided IDs. Clients do not get to choose which
messages are pulled, although Pub/Sub does attempt to provide oldest
messages first, with no hard guarantees around this.</p>
<p>To build a heuristic, we make some assumptions about the source that is
sending data into Pub/Sub. Specifically, we assume that the timestamps of the
original data are &#x201C;well behaved&#x201D;; in other words, we expect a bounded
amount of out-of-order timestamps on the source data, before it is sent to
Pub/Sub. Any data that are sent with timestamps outside the allowed out-of-
order bounds will be considered late data. In our current implementation, this
bound is at least 10 seconds, meaning reordering of timestamps up to 10
seconds before sending to Pub/Sub will not create late data. We call this
value the <em>estimation band</em>. Another way to look at this is that when the
pipepline is perfectly caught up with the input, the watermark will be 10
seconds behind real time to allow for possible reorderings from the source. If
the pipeline is backlogged, all of the backlog (not just the 10-second band) is
used for estimating the watermark.</p>
<p>What are the challenges we face with Pub/Sub? Because Pub/Sub does not
guarantee ordering, we must have some kind of additional metadata to know
enough about the backlog. Luckily, Pub/Sub provides a measurement of
backlog in terms of the &#x201C;oldest unacknowledged publish timestamp.&#x201D; This is
not the same as the event timestamp of our message, because Pub/Sub is</p>
<p>agnostic to the application-level metadata being sent through it; instead, this
is the timestamp of when the message was ingested by Pub/Sub.</p>
<p>This measurement is not the same as an event-time watermark. It is in fact the
processing-time watermark for Pub/Sub message delivery. The Pub/Sub
publish timestamps are not equal to the event timestamps, and in the case that
historical (past) data are being sent, it might be arbitrarily far away. The
ordering on these timestamps might also be different because, as mentioned
earlier, we allow a limited amount of reordering.</p>
<p>However, we can use this as a measure of backlog to learn enough
information about the event timestamps present in the backlog so that we can
create a reasonable watermark as follows.</p>
<p>We create two subscriptions to the topic containing the input messages: a
<em>base subscription</em> that the pipeline will actually use to read the data to be
processed, and a <em>tracking subscription</em> , which is used for metadata only, to
perform the watermark estimation.</p>
<p>Taking a look at our base subscription in Figure 3-19, we see that messages
might arrive out of order. We label each message with its Pub/Sub publish
timestamp &#x201C;pt&#x201D; and its event-time timestamp &#x201C;et.&#x201D; Note that the two time
domains can be unrelated.</p>
<pre><code>Figure 3-19. Processing-time and event-time timestamps of messages arriving on a
Pub/Sub subscription
</code></pre><p>Some messages on the base subscription are unacknowledged forming a</p>
<p>backlog. This might be due to them not yet being delivered or they might
have been delivered but not yet processed. Remember also that pulls from
this subscription are distributed across multiple shards. Thus, it is not
possible to say just by looking at the base subscription what our watermark
should be.</p>
<p>The tracking subscription, seen in Figure 3-20, is used to effectively inspect
the backlog of the base subscription and take the minimum of the event
timestamps in the backlog. By maintaining little or no backlog on the
tracking subscription, we can inspect the messages ahead of the base
subsciption&#x2019;s oldest unacknowledged message.</p>
<pre><code>Figure 3-20. An additional &#x201C;tracking&#x201D; subscription receiving the same messages as the
&#x201C;base&#x201D; subscription
</code></pre><p>We stay caught up on the tracking subscription by ensuring that pulling from
this subscription is computationally inexpensive. Conversely, if we fall
sufficiently behind on the tracking subscription, we will stop advancing the
watermark. To do so, we ensure that at least one of the following conditions
is met:</p>
<pre><code>The tracking subscription is sufficiently ahead of the base
subscription. Sufficiently ahead means that the tracking subscription
is ahead by at least the estimation band. This ensures that any
bounded reorder within the estimation band is taken into account.
</code></pre><pre><code>The tracking subscription is sufficiently close to real time. In other
words, there is no backlog on the tracking subscription.
</code></pre><p>We acknowledge the messages on the tracking subscription as soon as
possible, after we have durably saved metadata about the publish and event
timestamps of the messages. We store this metadata in a sparse histogram
format to minimize the amount of space used and the size of the durable
writes.</p>
<p>Finally, we ensure that we have enough data to make a reasonable watermark
estimate. We take a band of event timestamps we&#x2019;ve read from our tracking
subscription with publish timestamps newer than the oldest unacknowledged
of the base subscription, or the width of the estimation band. This ensures
that we consider all event timestamps in the backlog, or if the backlog is
small, the most recent estimation band, to make a watermark estimate.</p>
<p>Finally, the watermark value is computed to be the minimum event time in
the band.</p>
<p>This method is correct in the sense that all timestamps within the reordering
limit of 10 seconds at the input will be accounted for by the watermark and
not appear as late data. However, it produces possibly an overly conservative
watermark, one that advances &#x201C;too slowly&#x201D; in the sense described in
Chapter 2. Because we consider all messages ahead of the base subscription&#x2019;s
oldest unacknowledged message on the tracking subscription, we can include
event timestamps in the watermark estimate for messages that have already
been acknowledged.</p>
<p>Additionally, there are a few heuristics to ensure progress. This method
works well in the case of dense, frequently arriving data. In the case of sparse
or infrequent data, there might not be enough recent messages to build a
reasonable estimate. In the case that we have not seen data on the
subscription in more than two minutes (and there&#x2019;s no backlog), we advance
the watermark to near real time. This ensures that the watermark and the
pipeline continue to make progress even if no more messages are
forthcoming.</p>
<p>All of the above ensures that as long as source data-event timestamp
reordering is within the estimation band, there will be no additional late data.</p>

                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="section1.2.html" class="navigation navigation-prev " aria-label="Previous page: Section2.2 Batch Foundations: What and Where">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="summary.html" class="navigation navigation-next " aria-label="Next page: Summary">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"Going Streaming: When and How","level":"1.4.3","depth":2,"next":{"title":"Summary","level":"1.4.4","depth":2,"path":"chapter2/summary.md","ref":"chapter2/summary.md","articles":[]},"previous":{"title":"Section2.2 Batch Foundations: What and Where","level":"1.4.2","depth":2,"path":"chapter2/section1.2.md","ref":"chapter2/section1.2.md","articles":[]},"dir":"ltr"},"config":{"gitbook":"*","theme":"default","variables":{},"plugins":["livereload"],"pluginsConfig":{"livereload":{},"highlight":{},"search":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"sharing":{"facebook":true,"twitter":true,"google":false,"weibo":false,"instapaper":false,"vk":false,"all":["facebook","google","twitter","weibo","instapaper"]},"fontsettings":{"theme":"white","family":"sans","size":2},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"}},"file":{"path":"chapter2/summary0.md","mtime":"2019-10-04T01:21:27.000Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2019-10-03T07:23:23.538Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-livereload/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

